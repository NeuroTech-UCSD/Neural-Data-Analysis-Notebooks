{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG Feature Extraction 2: Parameterizing PSDs and Modeling\n",
    "In 2008, a BCI Competition was held on EEG datasets to find the best ML and statistical algorithms to differentiate different classes of neural data. The BCI Competition IV 2b is a motor imagery dataset with eye artifact data, making it a very realistic dataset. The subjects are prompted to imagine left vs right hand movement and the EEG + EOG signals for each trial are collected. We here have provided a simpler version of the dataset in CSV format for you to get started with. \n",
    "\n",
    "This notebook will introduce a couple more feature extraction methods, provide some guidance in dimensionality reduction, and show how these features can be used in a model to predict classes. \n",
    "\n",
    "Terminology: <br>\n",
    "- <b>Fitting Oscillations and One Over F (FOOOF)</b>: A technique that fits a 1/f model to the Power Spectrum Density to allow for more features to be extracted. \n",
    "- <b>Modeling and NN</b>: Machine learning techniques that can help us classify our data. \n",
    "\n",
    "Experiment Setup: http://www.bbci.de/competition/iv/desc_2b.pdf <br>\n",
    "FOOOF: https://fooof-tools.github.io/fooof/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path # For making paths compatible on Windows and Macs\n",
    "\n",
    "import pickle # For loading and creating pickle files\n",
    "import pandas as pd # For working with DataFrames \n",
    "import numpy as np # For ease of array manipulation, stats, and some feature extraction\n",
    "import matplotlib.pyplot as plt # For plotting pretty plots :) \n",
    "import scipy.signal as signal # For calculating PSDs and plotting spectrograms\n",
    "\n",
    "from neurodsp.spectral import compute_spectrum # For smoothed PSD computation\n",
    "from neurodsp.timefrequency import amp_by_time, freq_by_time, phase_by_time # For neurodsp features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_fs = 250 # Data was recorded at 250 Hz\n",
    "eeg_chans = [\"C3\", \"Cz\", \"C4\"] # 10-20 system \n",
    "eog_chans = [\"EOG:ch01\", \"EOG:ch02\", \"EOG:ch03\"] \n",
    "all_chans = eeg_chans + eog_chans\n",
    "event_types = {0:\"left\", 1:\"right\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Frequencies and mean PSDs from EEG data - this yeilds smoother PSDs because it averages the PSDs made from sliding windows. \n",
    "def getMeanFreqPSD(eeg_data, fs=eeg_fs):\n",
    "    freq_mean, psd_mean = compute_spectrum(eeg_data, fs, method='welch', avg_type='mean', nperseg=fs*2)\n",
    "    return freq_mean, psd_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Epoched Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These + epoched_test.pkl are the epochs that will be used in accuracy evaluation\n",
    "epoch_df_filename = Path(\"./data/epoched_train.pkl\")\n",
    "eeg_epoch_full_df = pd.read_pickle(epoch_df_filename)\n",
    "eeg_epoch_full_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Features from last time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save your feature_df from last time, feature_df.to_pickle(\"W1_feature_df.pkl\") in your 01_Notebook\n",
    "W1_feature_df_filename = Path(\"./W1_feature_df.pkl\")\n",
    "W1_feature_df = pd.read_pickle(W1_feature_df_filename)\n",
    "W1_feature_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's add a couple more features! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuroDSP Alpha Instantaneous Amplitude median\n",
    "Perhaps this can help account for large outliers and noise in the alpha power. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_range = (7, 12)\n",
    "alpha_amps = {}\n",
    "for i in range(0, len(eeg_epoch_full_df)): \n",
    "    for ch in eeg_chans: \n",
    "        amp = amp_by_time(eeg_epoch_full_df[ch][i][:], eeg_fs, alpha_range)\n",
    "        key = ch + \"_\" + str(alpha_range) + \"_inst_med\"\n",
    "        if key not in alpha_amps: \n",
    "            alpha_amps[key] = list()\n",
    "        alpha_amps[key].append(np.nanmedian(amp))\n",
    "\n",
    "alpha_med_df = pd.DataFrame(alpha_amps)\n",
    "display(alpha_med_df.head(2))\n",
    "\n",
    "feature_df = pd.concat([W1_feature_df, alpha_med_df], axis=1)\n",
    "display(feature_df.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOOOF (i.e. Fitting Oscillations and One Over F) \n",
    "PSDs are highly analyzed in neural data, and FOOOF helps parameterize some qualities of these PSDs that can allow us to consider more features of our data. In particular, this package will take a PSD and try to fit a 1/f aperiodic line to represent the base \"noise\" and then fit more curves above the line to outline the periodic activity. This can be helpful in being more specific about the oscillatory behavior in the neural data. \n",
    "\n",
    "Donoghue T, Haller M, Peterson E, Varma P, Sebastian P, Gao R, Noto T, Lara AH, Wallis JD,\n",
    "Knight RT, Shestyuk A, Voytek B. Parameterizing neural power spectra into periodic\n",
    "and aperiodic components. Nature Neuroscience (in press)\n",
    "\n",
    "They have great documentation and tutorials for fine-tuning the models to best parameterize your data on their website: https://fooof-tools.github.io/fooof/auto_tutorials/index.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first understand visually what the model does: \n",
    "Here we can see what the different features of the PSDs are identified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required code for visualizing example models\n",
    "from fooof import FOOOF\n",
    "from fooof.sim.gen import gen_power_spectrum\n",
    "from fooof.sim.utils import set_random_seed\n",
    "from fooof.plts.annotate import plot_annotated_model\n",
    "\n",
    "# Set random seed, for consistency generating simulated data\n",
    "set_random_seed(10)\n",
    "\n",
    "# Simulate example power spectra\n",
    "freqs1, powers1 = gen_power_spectrum([3, 40], [1, 1],\n",
    "                                     [[10, 0.2, 1.25], [30, 0.15, 2]])\n",
    "\n",
    "# Initialize power spectrum model objects and fit the power spectra\n",
    "fm1 = FOOOF(min_peak_height=0.05, verbose=False)\n",
    "fm1.fit(freqs1, powers1)\n",
    "\n",
    "plot_annotated_model(fm1, annotate_aperiodic=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see what the model predicts of one EEG trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants \n",
    "freq_range = [1, 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a FOOOF object\n",
    "fm = FOOOF(peak_width_limits=[1, 8], max_n_peaks=6, min_peak_height=0.4)\n",
    "\n",
    "# Get the PSD of our EEG Signal\n",
    "sig = eeg_epoch_full_df['Cz'][0]\n",
    "freq, psd = getMeanFreqPSD(sig)\n",
    "\n",
    "fm.add_data(freq, psd, freq_range)\n",
    "\n",
    "fm.fit()\n",
    "fm.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see what the model detects from the averaged PSDs that we saw before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get PSD averages for each channel for each event type (0=left or 1=right)\n",
    "psd_averages_by_type = {}\n",
    "\n",
    "for event_type in event_types.keys(): \n",
    "    psds_only_one_type={}\n",
    "    freqs_only_one_type={}\n",
    "    for i, row in eeg_epoch_full_df[eeg_epoch_full_df[\"event_type\"] == event_type].iterrows(): \n",
    "        for ch in eeg_chans: \n",
    "            if ch not in psds_only_one_type: \n",
    "                psds_only_one_type[ch] = list()\n",
    "                freqs_only_one_type[ch] = list()\n",
    "            f, p = getMeanFreqPSD(row[ch])\n",
    "            psds_only_one_type[ch].append(p)\n",
    "            freqs_only_one_type[ch].append(f)\n",
    "    avg_psds_one_type = {}\n",
    "    for ch in eeg_chans:\n",
    "        psds_only_one_type[ch] = np.array(psds_only_one_type[ch])\n",
    "        avg_psds_one_type[ch] = np.mean(psds_only_one_type[ch], axis=0)\n",
    "    psd_averages_by_type[event_type] = dict(avg_psds_one_type)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the parameters in these two classes of C4 activity\n",
    "fm_left_C4 = FOOOF(peak_width_limits=[1, 8], max_n_peaks=6, min_peak_height=0.4)\n",
    "fm_left_C4.add_data(freqs_only_one_type[eeg_chans[0]][0], psd_averages_by_type[0]['C4'], freq_range)\n",
    "fm_left_C4.fit()\n",
    "fm_left_C4.report()\n",
    "\n",
    "fm_right_C4 = FOOOF(peak_width_limits=[1, 8], max_n_peaks=6, min_peak_height=0.4)\n",
    "fm_right_C4.add_data(freqs_only_one_type[eeg_chans[0]][0], psd_averages_by_type[1]['C4'], freq_range)\n",
    "fm_right_C4.fit()\n",
    "fm_right_C4.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate central freq, alpha power, and bandwidth for each channel and each trial\n",
    "# This cell takes a few minutes to run (~8 mins on my computer). There are 3680 trials in the training data. \n",
    "\n",
    "# Initialize a fooof object\n",
    "fm = FOOOF(peak_width_limits=[1, 8], max_n_peaks=6, min_peak_height=0.4)\n",
    "\n",
    "# Some will not have alpha peaks, use these variables to keep track\n",
    "num_with_alpha = 0\n",
    "num_without_alpha = 0\n",
    "fooof_parameters = {}\n",
    "for i in range(len(eeg_epoch_full_df)):\n",
    "    # Print the trial number every 100 to make sure we're making progress\n",
    "    if i % 100 == 0 :\n",
    "        print(i)\n",
    "    for ch in eeg_chans:\n",
    "        # Determine the key\n",
    "        CF_key = ch + \"_alpha_central_freq\"\n",
    "        PW_key = ch + \"_alpha_power\"\n",
    "        BW_key = ch + \"_alpha_band_width\"\n",
    "        if CF_key not in fooof_parameters: \n",
    "            fooof_parameters[CF_key] = []\n",
    "        if PW_key not in fooof_parameters: \n",
    "            fooof_parameters[PW_key] = []\n",
    "        if BW_key not in fooof_parameters: \n",
    "            fooof_parameters[BW_key] = []\n",
    "        \n",
    "        # Calculate the PSD for the desired signal\n",
    "        sig = eeg_epoch_full_df[ch][i]\n",
    "        freq, psd = getMeanFreqPSD(sig)\n",
    "        \n",
    "        # Set the frequency and spectral data into the FOOOF model and get peak params\n",
    "        fm.add_data(freq, psd, freq_range)\n",
    "        fm.fit()\n",
    "        peak_params = fm.peak_params_\n",
    "        \n",
    "        # Only select the peaks within alpha power\n",
    "        peak_params_alpha = []\n",
    "        for param in peak_params: \n",
    "            if (param[0] > alpha_range[0]) and (param[0] < alpha_range[1]): \n",
    "                peak_params_alpha.append(param)\n",
    "        \n",
    "        # Take the average if there are multiple peaks detected, otherwise 0 everything\n",
    "        means = []\n",
    "        if len(peak_params_alpha) > 0: \n",
    "            num_with_alpha += 1\n",
    "            means = np.mean(peak_params_alpha, axis=0)\n",
    "        else :\n",
    "            num_without_alpha += 1\n",
    "            means = [0, 0, 0]\n",
    "        \n",
    "        fooof_parameters[CF_key].append(means[0])\n",
    "        fooof_parameters[PW_key].append(means[1])\n",
    "        fooof_parameters[BW_key].append(means[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate \n",
    "fooof_parameters_df = pd.DataFrame(fooof_parameters)\n",
    "display(fooof_parameters_df.head(5))\n",
    "\n",
    "feature_df = pd.concat([W1_feature_df, alpha_med_df, fooof_parameters_df], axis=1)\n",
    "display(feature_df.head(2))\n",
    "print(\"% with alpha:\", num_with_alpha / (num_with_alpha + num_without_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it so you don't need to spend extra time rerunning the heavy computation cell from above. \n",
    "feature_df.to_pickle(\"W2_feature_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now you have many features to put into your Machine Learning algorithm or plot with statistics to find differences between the classes! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, MaxPooling2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pickle.load(open(\"W2_feature_df.pkl\", \"rb\"))\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null \n",
    "null_df = pd.DataFrame(feature_df.isnull().mean(), columns=[\"null %\"])\n",
    "null_cols = null_df[null_df[\"null %\"] > 0]\n",
    "null_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View cleaned dataframe\n",
    "cleaned_feature_df = feature_df.drop(list(null_cols.index), axis=1)\n",
    "cleaned_feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(cleaned_feature_df.drop(\"y\", axis=1), cleaned_feature_df[\"y\"])\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model.fit(x_train, y_train)\n",
    "train_preds = log_model.predict(x_train)\n",
    "test_preds = log_model.predict(x_test)\n",
    "print(\"Training Accuracy: %f\" % accuracy_score(train_preds, y_train))\n",
    "print(\"Training F1 Score: %f\" % f1_score(train_preds, y_train))\n",
    "print(\"Testing Accuracy: %f\" % accuracy_score(test_preds, y_test))\n",
    "print(\"Testing F1 Score: %f\" % f1_score(test_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_model = DecisionTreeClassifier()\n",
    "dtree_model.fit(x_train, y_train)\n",
    "train_preds = dtree_model.predict(x_train)\n",
    "test_preds = dtree_model.predict(x_test)\n",
    "print(\"Training Accuracy: %f\" % accuracy_score(train_preds, y_train))\n",
    "print(\"Training F1 Score: %f\" % f1_score(train_preds, y_train))\n",
    "print(\"Testing Accuracy: %f\" % accuracy_score(test_preds, y_test))\n",
    "print(\"Testing F1 Score: %f\" % f1_score(test_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = RandomForestClassifier()\n",
    "forest_model.fit(x_train, y_train)\n",
    "train_preds = forest_model.predict(x_train)\n",
    "test_preds = forest_model.predict(x_test)\n",
    "print(\"Training Accuracy: %f\" % accuracy_score(train_preds, y_train))\n",
    "print(\"Training F1 Score: %f\" % f1_score(train_preds, y_train))\n",
    "print(\"Testing Accuracy: %f\" % accuracy_score(test_preds, y_test))\n",
    "print(\"Testing F1 Score: %f\" % f1_score(test_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_model = AdaBoostClassifier()\n",
    "adaboost_model.fit(x_train, y_train)\n",
    "train_preds = adaboost_model.predict(x_train)\n",
    "test_preds = adaboost_model.predict(x_test)\n",
    "print(\"Training Accuracy: %f\" % accuracy_score(train_preds, y_train))\n",
    "print(\"Training F1 Score: %f\" % f1_score(train_preds, y_train))\n",
    "print(\"Testing Accuracy: %f\" % accuracy_score(test_preds, y_test))\n",
    "print(\"Testing F1 Score: %f\" % f1_score(test_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = SVC()\n",
    "svc_model.fit(x_train, y_train)\n",
    "train_preds = svc_model.predict(x_train)\n",
    "test_preds = svc_model.predict(x_test)\n",
    "print(\"Training Accuracy: %f\" % accuracy_score(train_preds, y_train))\n",
    "print(\"Training F1 Score: %f\" % f1_score(train_preds, y_train))\n",
    "print(\"Testing Accuracy: %f\" % accuracy_score(test_preds, y_test))\n",
    "print(\"Testing F1 Score: %f\" % f1_score(test_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_cv = GridSearchCV(AdaBoostClassifier(),\n",
    "                          param_grid={\"n_estimators\":range(30, 101, 10),\n",
    "                                     \"learning_rate\":[1, 0.1, 0.01]})\n",
    "adaboost_cv.fit(x_train, y_train)\n",
    "best_model = adaboost_cv.best_estimator_\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = best_model.predict(x_train)\n",
    "test_preds = best_model.predict(x_test)\n",
    "print(\"Training Accuracy: %f\" % accuracy_score(train_preds, y_train))\n",
    "print(\"Training F1 Score: %f\" % f1_score(train_preds, y_train))\n",
    "print(\"Testing Accuracy: %f\" % accuracy_score(test_preds, y_test))\n",
    "print(\"Testing F1 Score: %f\" % f1_score(test_preds, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load epoched EEG data for Neural Net training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pickle.load(open(\"data/epoched_train.pkl\", \"rb\"))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop([\"patient_id\", \"start_time\", \"event_type\"], axis=1).iloc[0].apply(lambda x:x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "Looking at the raw data's shapes above, we can concatenate the 6 channels to create an input of 6000 values into a basic neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop([\"patient_id\", \"start_time\", \"event_type\"], axis=1).apply(lambda x:np.concatenate(x), axis=1)\n",
    "X = np.array(X.values.tolist())\n",
    "Y = train_df[\"event_type\"].values.astype(float)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network = keras.Sequential([Dense(2048, activation=\"relu\"),\n",
    "                                   Dense(1024, activation=\"relu\"),\n",
    "                                   Dense(512, activation=\"relu\"),\n",
    "                                   Dense(128, activation=\"relu\"),\n",
    "                                   Dense(1, activation=\"sigmoid\")])\n",
    "opt = Adam(lr=0.001)\n",
    "neural_network.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = neural_network.fit(x_train, y_train, epochs=15, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 4))\n",
    "axs[0].plot(history.history[\"loss\"])\n",
    "axs[0].set_title(\"Neural Network Training Loss\")\n",
    "axs[0].set_ylabel(\"Categorial Crossentropy\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[1].plot(history.history[\"accuracy\"])\n",
    "axs[1].set_title(\"Neural Network Training Accuracy\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[1].set_xlabel(\"Epoch\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_network.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "To use a convolutional neural network architecture, we will instead stack the 1000 dimensional arrays to make 1000 x 6 matrices that can the model can perform convolution on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop([\"patient_id\", \"start_time\", \"event_type\"], axis=1).apply(lambda x:np.stack(x, axis=-1), axis=1)\n",
    "X = np.array(X.values.tolist())\n",
    "X = X.reshape(list(X.shape)+[1])\n",
    "Y = train_df[\"event_type\"].values.astype(float)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = keras.Sequential([Conv2D(32, (3, 3), activation=\"relu\", input_shape=(1000, 6, 1)),\n",
    "                        Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "                        Flatten(),\n",
    "                        Dense(256, activation=\"relu\"),\n",
    "                        Dropout(0.2),\n",
    "                        Dense(128, activation=\"relu\"),\n",
    "                        Dense(1, activation=\"sigmoid\")])\n",
    "opt = Adam(lr=0.001)\n",
    "cnn.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn.fit(x_train, y_train, epochs=8, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 4))\n",
    "axs[0].plot(history.history[\"loss\"])\n",
    "axs[0].set_title(\"CNN Training Loss\")\n",
    "axs[0].set_ylabel(\"Categorial Crossentropy\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[1].plot(history.history[\"accuracy\"])\n",
    "axs[1].set_title(\"CNN Training Accuracy\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[1].set_xlabel(\"Epoch\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
